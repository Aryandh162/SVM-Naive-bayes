{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## SVM & Naive bayes"
      ],
      "metadata": {
        "id": "HMNf2tavQ6Nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM)?"
      ],
      "metadata": {
        "id": "F_AWck5rRDmU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> A Support Vector Machine (SVM) is a supervised machine learning algorithm that can be used for both classification and regression tasks. In classification, SVM aims to find an optimal hyperplane that separates data points of different classes with the largest possible margin."
      ],
      "metadata": {
        "id": "x4_aFELYiwK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Hard Margin and Soft Margin SVM?"
      ],
      "metadata": {
        "id": "r7MxNyQeRhsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Hard Margin SVM: This type of SVM seeks to find a hyperplane that perfectly separates the data points into their respective classes without allowing any misclassifications. It assumes that the data is linearly separable. While it provides a clear separation, it is sensitive to outliers and noise in the data.\n",
        "\n",
        "Soft Margin SVM: In contrast, Soft Margin SVM allows for some misclassifications to occur. It introduces a regularization parameter (C) that controls the trade-off between maximizing the margin and minimizing the classification errors. This makes Soft Margin SVM more robust to noisy data and non-linearly separable datasets."
      ],
      "metadata": {
        "id": "KUNmT9PUi16w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the mathematical intuition behind SVM?"
      ],
      "metadata": {
        "id": "cYa0d8NlRkGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> The mathematical intuition behind SVM is to find the optimal hyperplane that maximizes the margin between the different classes. The margin is defined as the distance between the hyperplane and the nearest data point from either class (the support vectors)."
      ],
      "metadata": {
        "id": "PujACqK_i84J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the role of Lagrange Multipliers in SVM?"
      ],
      "metadata": {
        "id": "xKe4IUN1RmMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Lagrange multipliers are used in SVM to solve the constrained optimization problem of finding the optimal hyperplane. The problem is to maximize the margin subject to the constraint that all data points are correctly classified (in the case of hard margin SVM) or with a controlled number of misclassifications (in the case of soft margin SVM)."
      ],
      "metadata": {
        "id": "irrunZAtjC6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are Support Vectors in SVM?"
      ],
      "metadata": {
        "id": "Zz7nAOrdRoTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Support vectors are the data points that lie closest to the hyperplane in SVM. These are the points that are most difficult to classify and directly influence the position and orientation of the optimal hyperplane. In both hard and soft margin SVM, only the support vectors are needed to define the hyperplane, making SVM memory-efficient once trained."
      ],
      "metadata": {
        "id": "y3AFErOajIML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is a Support Vector Classifier (SVC)?"
      ],
      "metadata": {
        "id": "PVfphJozRqLP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=>  Support Vector Classifier (SVC), the support vectors are the data points that lie closest to the decision boundary (hyperplane). These are the critical instances that determine the position and orientation of the hyperplane. Only the support vectors are needed to define the SVC model, making it efficient in terms of memory usage after training."
      ],
      "metadata": {
        "id": "REOGyD3GjVDo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. What is a Support Vector Regressor (SVR)?"
      ],
      "metadata": {
        "id": "zeGN-DQtRsH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> A Support Vector Regressor (SVR) is a type of Support Vector Machine (SVM) used for regression tasks. While SVMs are commonly associated with classification, they can be extended to handle regression problems."
      ],
      "metadata": {
        "id": "ehNZE3fRjjrI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Kernel Trick in SVM?\n"
      ],
      "metadata": {
        "id": "B0xCJ1BHRt2G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> The Kernel Trick is a powerful technique used in Support Vector Machines (SVMs) to handle non-linearly separable data without explicitly transforming the data into a higher-dimensional feature space."
      ],
      "metadata": {
        "id": "vUmtc-7yjr7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel."
      ],
      "metadata": {
        "id": "GE2QHgg5RxeF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Key Differences and Considerations:\n",
        "\n",
        "1. Linear Kernel: The simplest kernel, suitable for data that is linearly separable. It's a good starting point and provides a baseline for comparison.\n",
        "2. Polynomial Kernel: Allows for non-linear decision boundaries that are polynomial in shape. The degree parameter is crucial and needs to be tuned. A higher degree can fit more complex patterns but increases the risk of overfitting.\n",
        "3. RBF Kernel: A very flexible kernel that can create complex, non-linear decision boundaries. It's often a good default choice because it can handle a wide range of data patterns. The gamma parameter controls the influence of individual training examples; a larger gamma leads to a more complex decision boundary and can cause overfitting."
      ],
      "metadata": {
        "id": "84Pxj5k_jy0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the effect of the C parameter in SVM?\n"
      ],
      "metadata": {
        "id": "NWdoeLuBRzPh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> The C parameter in SVM is a regularization parameter that controls the trade-off between achieving a large margin and minimizing the classification errors. It is specifically used in the context of Soft Margin SVM.\n"
      ],
      "metadata": {
        "id": "0xaQiskhj8Bz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is the role of the Gamma parameter in RBF Kernel SVM?"
      ],
      "metadata": {
        "id": "BR0f6wmCR2zl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> The gamma parameter in SVM with the Radial Basis Function (RBF) kernel controls the influence of individual training samples. It effectively defines how far the influence of a single training example reaches."
      ],
      "metadata": {
        "id": "Gs-jwOXKkwoO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?"
      ],
      "metadata": {
        "id": "C4gHwW0eR4lW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> The Naïve Bayes classifier is a probabilistic machine learning algorithm based on Bayes' Theorem. It's primarily used for classification tasks.\n",
        "\n",
        "Here's a breakdown of what it is and why it's called \"Naïve\":\n",
        "\n",
        "What is Naïve Bayes?\n",
        "\n",
        "It's a supervised learning algorithm used for both binary and multi-class classification.\n",
        "It calculates the probability of a data point belonging to a certain class based on the values of its features."
      ],
      "metadata": {
        "id": "ybrzOfnek5R9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is Bayes’ Theorem?"
      ],
      "metadata": {
        "id": "psEcIwFVR6b2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Bayes' Theorem is a fundamental theorem in probability theory that describes how to update the probability of a hypothesis based on new evidence. It's the mathematical foundation of the Naïve Bayes classifier."
      ],
      "metadata": {
        "id": "pmD2hox_lCcF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes?"
      ],
      "metadata": {
        "id": "yzL20M1FR8-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Key Differences Summarized:\n",
        "\n",
        "* Gaussian Naïve Bayes is for continuous numerical features and assumes a Gaussian distribution for these features within each class.\n",
        "* Multinomial Naïve Bayes is for discrete count features and is well-suited for data like text where features are counts (e.g., word frequencies).\n",
        "* Bernoulli Naïve Bayes is for binary features, where features represent the presence or absence of something (e.g., a word appearing in a document)."
      ],
      "metadata": {
        "id": "opqI2jUrlJv7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. When should you use Gaussian Naïve Bayes over other variants?"
      ],
      "metadata": {
        "id": "ZV0osnOtR-0l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Gaussian Naïve Bayes when your dataset contains continuous numerical features and you assume that these features follow a Gaussian (normal) distribution within each class."
      ],
      "metadata": {
        "id": "1G1hj50plULZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the key assumptions made by Naïve Bayes?"
      ],
      "metadata": {
        "id": "YKpUk4oeSAgs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Naïve Bayes can still perform well for several reasons:\n",
        "\n",
        "* Classification vs. Probability Estimation: Even if the probability estimates are not perfectly accurate due to the independence assumption, the classification decision (which class has the highest probability) can still be correct.\n",
        "* Simplicity and Robustness: The simplicity of the model makes it less prone to overfitting, especially in high-dimensional spaces.\n",
        "* Efficiency: The independence assumption simplifies the calculations and makes the algorithm very efficient, particularly for large datasets."
      ],
      "metadata": {
        "id": "i1N05z4klfxS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the advantages and disadvantages of Naïve Bayes?"
      ],
      "metadata": {
        "id": "1SIvPJX9SCMV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Advantages:\n",
        "\n",
        "* Simplicity and Ease of Implementation: Naïve Bayes is conceptually simple and straightforward to implement. Its probabilistic nature makes it intuitive to understand.\n",
        "* Computational Efficiency: It is very fast to train and predict, especially for large datasets. The calculation of probabilities is efficient due to the independence assumption.\n",
        "* Works Well with High-Dimensional Data: Despite the independence assumption, Naïve Bayes often performs well on datasets with a large number of features, such as text classification problems.\n",
        "* Requires Less Training Data (relatively): Compared to some other algorithms, Naïve Bayes can perform reasonably well even with a moderate amount of training data, particularly if the independence assumption is not severely violated.\n",
        "* Good for Text Classification: It has been historically and remains a strong and popular choice for text classification tasks.\n",
        "\n",
        "Disadvantages:\n",
        "\n",
        "* The \"Naïve\" Independence Assumption: The core assumption that features are conditionally independent given the class is often not true in real-world datasets. This can lead to inaccurate probability estimates, although it doesn't always negatively impact the final classification decision.\n",
        "* Zero Probability Problem: If a feature value does not appear in the training data for a particular class, the probability of that feature given the class will be zero. This can cause the entire posterior probability for that class to become zero, regardless of the other features. Techniques like Laplace smoothing can mitigate this.\n",
        "* Poor Estimator of Probabilities: While Naïve Bayes can be a good classifier, the probability outputs it generates may not be well-calibrated. They might not accurately reflect the true probabilities.\n",
        "* Sensitive to Data Distribution: The performance can be affected if the distribution of features within classes significantly deviates from the assumed distribution (e.g., not Gaussian for Gaussian Naïve Bayes)."
      ],
      "metadata": {
        "id": "C9IN1zBulspx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Why is Naïve Bayes a good choice for text classification?"
      ],
      "metadata": {
        "id": "GoSMGB1mSD5n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> Text data, when represented using techniques like Bag-of-Words, results in sparse matrices where most entries are zero (most documents only contain a small fraction of the total vocabulary). Naïve Bayes handles this sparsity well. The calculations only involve non-zero feature counts, which is efficient for sparse data"
      ],
      "metadata": {
        "id": "3JGOUAu4mEZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. Compare SVM and Naïve Bayes for classification tasks."
      ],
      "metadata": {
        "id": "zCnaV8GvSFr9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> When to Use Which:\n",
        "\n",
        "Use SVM:\n",
        "\n",
        "1. When your data is not linearly separable and you can effectively use kernel functions.\n",
        "2. When you have a moderate to large dataset.\n",
        "3. When you are less concerned about the speed of training.\n",
        "4. When you want a powerful model that often achieves high accuracy.\n",
        "\n",
        "Use Naïve Bayes:\n",
        "\n",
        "1. When you have high-dimensional, sparse data, such as text data.\n",
        "2. When computational efficiency and speed are critical.\n",
        "3. When the independence assumption is a reasonable approximation or when it doesn't significantly harm performance in practice.\n",
        "4. As a strong baseline model to compare with more complex algorithms."
      ],
      "metadata": {
        "id": "ug3kRXoGmLhU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How does Laplace Smoothing help in Naïve Bayes?"
      ],
      "metadata": {
        "id": "0pQCRYlbSHV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "=> The simplicity of the model can make it less prone to overfitting compared to more complex models, especially when dealing with high-dimensional text data with limited training examples."
      ],
      "metadata": {
        "id": "humApI-vmb_7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical Questions"
      ],
      "metadata": {
        "id": "TYEfvld-SH83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy\n"
      ],
      "metadata": {
        "id": "uMcnSm0eSRbp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svm_clf.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the SVM classifier on the Iris dataset: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "hzlI02EEU4Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies."
      ],
      "metadata": {
        "id": "vg45HdPeTewi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "linear_svm = SVC(kernel='linear')\n",
        "\n",
        "linear_svm.fit(X_train, y_train)\n",
        "\n",
        "linear_y_pred = linear_svm.predict(X_test)\n",
        "\n",
        "linear_accuracy = accuracy_score(y_test, linear_y_pred)\n",
        "print(f\"Accuracy of the Linear SVM classifier on the Wine dataset: {linear_accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "AVRJjQBxU_KS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "y3Z62qlQTg1_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svr = SVR()\n",
        "\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "y_pred = svr.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error of the SVR on the Iris dataset: {mse:.2f}\")"
      ],
      "metadata": {
        "id": "zkgk7hSlU_78"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision boundary.\n"
      ],
      "metadata": {
        "id": "J71gM956Ti7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='poly', degree=3)\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ypTJaHqDVAbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy."
      ],
      "metadata": {
        "id": "WNYrP_-uTmAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "breast_cancer = datasets.load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "gnb = GaussianNB()\n",
        "\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Gaussian Naïve Bayes classifier on the Breast Cancer dataset: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "Hda02qioVA2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20 Newsgroups dataset."
      ],
      "metadata": {
        "id": "t06Nl9QFToOd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "X_test = vectorizer.transform(newsgroups_test.data)\n",
        "\n",
        "y_train = newsgroups_train.target\n",
        "y_test = newsgroups_test.target\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "\n",
        "mnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = mnb.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Multinomial Naïve Bayes classifier on the 20 Newsgroups dataset: {accuracy:.2f}\")"
      ],
      "metadata": {
        "id": "OUyOhtDVVBYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Write a Python program to train an SVM Classifier with different C values and compare the decision boundaries visually\n"
      ],
      "metadata": {
        "id": "FXc16jojTpqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cMcpp5DWVB9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with binary features\n"
      ],
      "metadata": {
        "id": "PHUcQKEZUM8t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rW9l4HfQVCkx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Write a Python program to apply feature scaling before training an SVM model and compare results with unscaled data\n"
      ],
      "metadata": {
        "id": "6uw6AVipUPSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EEoxjWv9VDFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and after Laplace Smoothing\n"
      ],
      "metadata": {
        "id": "D361RYy6URtt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gWs9Ry6vVDe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C, gamma, kernel)\n"
      ],
      "metadata": {
        "id": "O3mmYyPwUUr9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qEjCLft-VD5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and check it improve accuracy\n"
      ],
      "metadata": {
        "id": "qQqzm4dDUXRn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l_QgkIeDVEVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data\n"
      ],
      "metadata": {
        "id": "FUjyaj1WUZzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RZTJlTOeVE3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and compare their accuracy\n"
      ],
      "metadata": {
        "id": "U9kndSpxUb0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6eUzz5ghVFa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare results\n"
      ],
      "metadata": {
        "id": "Oz2Sx-2DUeF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "MN8kdR8zVFzL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO) strategies on the Wine dataset and compare their accuracy\n"
      ],
      "metadata": {
        "id": "wykH3QxqUggc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ijpn4qCPVGeZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast Cancer dataset and compare their accuracy."
      ],
      "metadata": {
        "id": "fkQzQL2uUioV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LZjo31jwVHBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the average accuracy\n"
      ],
      "metadata": {
        "id": "NN6ZI9EbUkri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ojBPGWbCVHeB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare performance\n"
      ],
      "metadata": {
        "id": "BRy9ULOAUm3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUMsm-tiVH-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and compare accuracy\n"
      ],
      "metadata": {
        "id": "4Z-3lKCtUo0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1PXmYOmqVJTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and F1-Score instead of accuracy\n"
      ],
      "metadata": {
        "id": "xVgaFx2qUq9M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YMbTs3_sVJ7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss (Cross-Entropy Loss)\n"
      ],
      "metadata": {
        "id": "dZT-N9ezUtCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WHxx8sDxVKUK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn."
      ],
      "metadata": {
        "id": "loDwswdrUu2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y5TBfpphVK2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute Error (MAE) instead of MSE.\n"
      ],
      "metadata": {
        "id": "HvJvQjivUw1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oatr69bYVLXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC score.\n"
      ],
      "metadata": {
        "id": "C3stiCNXUzWW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear')\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),\n",
        "                     np.arange(y_min, y_max, 0.02))\n",
        "Z = svm_clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.coolwarm)\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('SVM Decision Boundary')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m2xcTxC7n6Sb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve."
      ],
      "metadata": {
        "id": "94Grk_5cU2NO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#code for above ques.\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data[iris.target != 2]\n",
        "y = iris.target[iris.target != 2]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "svm_clf = SVC(kernel='linear', probability=True, random_state=42)\n",
        "\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "y_scores = svm_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f'Precision-Recall curve (AUC = {pr_auc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='lower left')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iT-NHlznVMSH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}